# Reinforcement Learning: Cliff Walking
## Overview
This repository contains the implementation of two fundamental reinforcement learning algorithms, **Q-learning** and **SARSA**, applied to the **Cliff Walking** environment. The project explores how these algorithms learn to navigate the gridworld, avoid the cliff, and reach the goal while minimizing penalties.

## Key Concepts
- **Parts:**
  - **Q-learning:** An off-policy algorithm that learns the optimal policy by estimating the maximum future rewards.
  - **SARSA:** An on-policy algorithm that updates its policy based on the actual actions taken, leading to potentially safer but less aggressive strategies.
  - **Comparison:** A detailed comparison of the paths chosen by each algorithm, highlighting differences in exploration and exploitation behaviors.

- **Tasks:**
  - Implement and evaluate the Q-learning algorithm.
  - Implement and evaluate the SARSA algorithm.
  - Compare and analyze the optimal policies derived from both algorithms.

## Usage
1. **Open the Jupyter Notebook:**
   - Locate and open `Practical_6_Reinforcement_Learning-2.ipynb`.
2. **Run the Notebook:**
   - Execute the cells in sequence to observe the implementation process, the learning outcomes, and the comparative analysis of Q-learning and SARSA.
